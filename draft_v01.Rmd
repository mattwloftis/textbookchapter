---
title: "'Big data' og politologisk datavidenskab"
author:
  - Frederik Hjorth
  - Matt W. Loftis
subtitle: "Udkast, april 2020"
date:
  # "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: true
  word_document:
    number_sections: true
biblio-style: apsr
fontfamily: cochineal
fontsize: 12pt
geometry: margin=1in
bibliography: references.bib
spacing: double
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

'Big data' er overalt. Det gælder i dobbelt forstand: takket være drastiske stigninger i computeres hukommelse og regnekraft indeholder næsten alle computere i dag store, ustrukturerede datamængder. Mange af disse data er biprodukter af menneskelig adfærd, som i dag registreres og kvantificeres i historisk uset omfang. Men big data er også overalt i den forstand at begrebet 'big data' og beslægtede begreber er blevet almindeligt kendte og bredt anvendte, og ikke mindst genstand for stor kommerciel interesse. En hyppigt citeret artikel fra *Harvard Business Review* kaldte således "data scientist" for "det 21. århundredes mest sexede job" [@davenport2012data].

Alene den begrebslige udbredelse af big data gør det relevant at vide hvad det nærmere dækker over. Men big data er også reelt et væsentligt nybrud i forhold til de data og metoder, politologi og samfundsvidenskab traditionelt har betjent sig af. Big data muliggør analyser af politologiske emner som ville have været umulige med traditionelle metoder, men kræver også nye teknikker og metodiske værktøjer.

Formålet med dette kapitel er at introducere til de datatyper og metoder, begrebet big data dækker over. Først opridser vi begrebets betydning og historie. Dernæst diskuterer vi hvordan en række karakteristika ved big data skaber særlige udfordringer i forhold til at udvikle stærke forskningsdesigns. Herefter præsenterer vi en række specifikke tekniske værktøjer til behandling af big data. Afslutningsvis opridser vi nogle væsentlige etiske problematikker i relation til brugen af big data.

<!-- ML: Should we say "en række specifikke tekniske værktøjer" Seems more like we'll present 'classes' of specific methods or 'families' of specific methods with references for further reading -->

# Hvad er big data?

I en toneangivende artikel peger @lazeretal på big data som kilden til en ny type samfundsvidenskab, "computational social science", med "kapacitet til at indsamle og analysere data med historisk uset bredde, dybde og omfang". Men begrebet big data lever to liv. På den ene side er den populære definition af begrebet forbundet med futuristiske løfter om ny, datadrevet videnskab og teknologi. På den anden side står hvad man kunne kalde den operationelle definition af hvordan store datamænger indsamles, lagres og analyseres af samfundsvidenskabsfolk. For at belyse betydningen af big data for politologi betragter vi først denne anden, operationelle betydning af begrebet inden vi vender tilbage til den første, populære betydning.

En bredt anvendt operationel definition identificerer big data med de såkaldte 'tre V'er': *Volume*, *Variety* og *Velocity*, dvs. omfang, variation og hastighed [@laney01]. Big data refererer her til foranderlige og meget store datasæt -- inklusive data der er for store til at gemme på en almindelig pc -- der indeholder masser af variation. Datakilder af denne art er ofte interessante for samfundsvidenskab: søgemaskinelogfiler, sociale medieaktiviteter, offentlige registre, mobiltelefonregistre eller endda data gemt ved passiv overvågning udført af digitale enheder i den fysiske verden er alle blevet anvendt til samfundsvidenskabelig forskning [@salganik17]. Adgang til disse data kræver partnerskaber med deres ejere -- telefonfirmaer, regeringer, teknologiselskaber osv. Dette kan indebære at man skriver software til at få adgang til websteder eller eksterne databaser eller kan involvere mere formaliserede partnerskaber for at dele information sikkert [@EL14]. I afsnittet om "Anskaffelse af data" nedenfor beskriver vi nærmere hvordan det kan finde sted.

Dette peger også på en vigtig forskel mellem big data og traditionelle samfundsvidenskabelige data: traditionelle samfundsvidenskabelige data er typisk indsamlet med samfundsvidenskab som formål. I modsætning hertil omtales big data til tider som "fundne data" eller "digital udstødning" [@harford14]. Hvad betyder det? Næsten alle big data er udviklet til *andre formål* end samfundsforskning. Metadata såsom tidsmarkører, antal følgere, eller aktivitetsmål på sociale medier lagres ikke for videnskabens skyld - eller nødvendigvis i overensstemmelse med videnskabelige standarder.

Selv offentlige datakilder kan udvise dette problem. Betragt for eksempel et lille, men illustrativt eksempel: lov nr. 1049 af 11/12/1996. Loven er ganske kort, færre end 20 ord. Det eneste den gør er at ophæve lov om skoleskibsafgift. Civilstyrelsens database med al dansk lovgivning, Retsinformation, angiver adskillige stykker metadata om lov nr. 1049 (jf. `https://www.retsinformation.dk/Forms/R0710.aspx?id=83509`). Metadata angiver f.eks. den lov der ophæves, adskillige relaterede dokumenter, lovens offentliggørelsesdato, og dens ministerområde. Men dette sidste datapunkt er lidt forvirrende. Ministerområdet for lov nr. 1049 er angivet som "Uddannelses- og Forskningsministeriet". Men loven er underskrevet af Mimi Jakobsen, som var erhvervsminister i regeringen Poul Nyrup Rasmussen II. Så hvorfor er loven ikke tilknyttet Erhvervsministeriet? Fejlen opstår fordi Civilstyrelsen opdaterer lovgivning løbende så den afspejler lovgivningens ressortområde *i dag*. Koblingen af lov nr. 1049 til Uddannelses- og Forskningsministeriet er indlysende forkert hvis du skal bruge historiske data om dansk lovgivning. Uheldigvis for politologer er det korrekt hvis du - som Civilstyrelsen - ikke har til formål at bedrive historisk forskning, men i stedet vil organisere gældende dansk lovgivning efter ministerområder. For at gøre ondt værre kan Retsinformation ændre sig yderligere i fremtiden på måder der ikke gavner politologien, alt efter hvad der tjener Civilstyrelsens behov.

Når man analyserer big data der er produceret til et eksisterende privat eller offentligt formål lurer problemer af denne type konstant. Selvom big data kan være enormt værdifulde for samfundsvidenskaben er deres værdi en *utilsigtet bivirkning* af kommerciel aktivitet eller myndighedsudøvelse. Når vi anvender big data i forskningsøjemed er det derfor altid vigtigt at forstå hvorfor og hvordan data er opstået til at begynde med, og tænke igennem hvilke implikationer det har for vores videnskabelige anvendelse. Som @salganik17 formulerer det følger både udfordringer og muligheder ved big data af at spørge sig selv hvorfor data blev indsamlet i første omgang.

 <!-- - Definition of machine learning: "y-hat vs. beta-hat" -->

# Kilder til big data

Det er ofte en møjsommelig proces at indsamle samfundsvidenskabelige data. Derfor fremhæves det ofte som en fordel ved big data at undersøgelsens subjekter selv genererer data: en forsker kan eksempelvis indsamle millioner af tweets om et politisk emne uden skulle uddele et eneste spørgeskema. Men selv om data er genereret på forhånd er det ikke ligetil at *anskaffe* sig data. Der er groft sagt tre måder man kan gøre det på.

## Scraping

Den første og mest umiddelbare måde er at udtrække data direkte fra websider, typisk kaldet *scraping*. Scraping udnytter at indholdet på de fleste større websider kommer fra databaser som fremstiller indholdet i websider med en konsistent struktur. Ved at hente kildekoden til disse websider, på samme måde som en webbrowser gør det, kan man udtrække data på en konsistent måde. Hvis man f.eks. besøger websiden for *Lov om ophævelse af lov om skoleskibsafgift*  hos Retsinformation finder man i sidens kildekode bl.a. dette:

```
<div class="metadata-summary">
			<span class="kortNavn">LOV nr 1049 af 11/12/1996 Gældende</span><br>
      <div class="ressort">
				Offentliggørelsesdato: 12-12-1996<br>
        Uddannelses- og Forskningsministeriet
			</div>
		</div>
```

Kodestumpen viser at Retsinformations database lagrer lovens navn i feltet `kortNavn` og lovens offentliggørelsesdato og ressortområde i feltet `ressort`. Takket være den stringente kodestruktur er det nemt at gemme disse og andre metadata i et analyserbart format. Og fordi kodestrukturen er ens på tværs af love hos Retsinformation kan man scrape data om tusindvis af andre love med samme lille stykke kode.

Når man indsamler data ved hjælp af scraping tilgår man i princippet data på samme måde som en almindelig internetbruger der benytter sig af en browser. Men fordi scraping gør det muligt at hente kolossale datamængder er det også en kontroversiel praksis. Et illustrativt eksempel på det kommer fra en meget omtalt juridisk strid mellem det sociale netværk LinkedIn og analysefirmaet HiQ. En del af HiQ's forretningsmodel er at analysere arbejdsmarkedet for it-specialister, og HiQ har bl.a. høstet data ved at scrape oplysninger om individuelle it-specialister fra offentlige profiler på LinkedIn. I 2017 sagsøgte LinkedIn HiQ med påstand om at HiQ's scraping-praksis var et brud på amerikansk it-lovgivning. HiQ fik til sidst medhold i at virksomheden kunne scrape data fra offentlige LinkedIn-sider uden tilsagn fra LinkedIn, men sagen illustrerer at scraping ofte finder sted i en juridisk gråzone.

Kodestumpen om *Lov om ophævelse af lov om skoleskibsafgift* i eksemplet ovenfor kommer fra Retsinformation, og det er som hovedregel ikke forbudt at scrape data fra offentlige hjemmesider, så længe man ikke urimeligt belaster udbyderens servere. Man bør dog uanset kilden altid sikre sig tilsagn fra dataudbyderen før man går i gang med at scrape data.

<!-- ML: This seems like a good place to mention a thing that really frustrates me! The robots.txt for Denmark's Supreme Court's decision database disallows all scraping: http://domstol.fe1.tangora.com/robots.txt I haven't tried poking the bear and doing it anyway, but I often use this as an example of when researchers have to make explicit trade-offs in their data collection. If the Supreme Court were to refuse a request to share its data, I think it would be thoroughly **ethical** to violate these scraping guidelines. But, at what cost to the researcher? -->


## API'er

En anden måde at hente data på er gennem såkaldte API'er. API står for *Application Programming Interface* og er en slags kontrolleret adgang til data hos en dataudbyder. API'er indebærer altså ikke samme juridiske usikkerheder som scraping, da udbyderen selv stiller data til rådighed og definerer rammerne herfor. Eksempelvis har mange API'er *rate limits* der sætter grænser for hvor meget data man kan hente ad gangen.

Princippet om at big data ikke er lavet for samfundsforskningens skyld gælder også for API'er. Det egentlige formål for de fleste API'er er at dele data på tværs af kommercielle platforme. For eksempel er der API'er der muliggør at et online-medie kan vise hvilke af ens egne Facebook-venner der har 'liket' en specifik artikel, fordi avisen kan tilgå data om læserens Facebook-netværk gennem Facebooks API. Men mange sociale netværk stiller meget righoldige data til rådighed for forskere gennem API'er. For eksempel bruger @hjorth2019, som studerer fordelingen af online misinformation, Twitters API til at indsamle data om ca. 13 millioner følgere af ca. 10.000 Twitter-konti. Offentlige myndigheder stiller også i stigende grad data til rådighed gennem API'er. Eksempelvis stiller Folketinget data om medlemmer, forhandlinger og lovarbejde til rådighed gennem en API.

## Datasamarbejder

En tredje måde at få adgang til big data er gennem et egentligt samarbejde med virksomheder der lagrer big data. For eksempel rapporterer @bond2012 om et eksperiment, hvor samfundsforskere i samarbejde med Facebook randomiserede hvilken type information Facebook-brugere fik om deres venners stemmeadfærd. I kraft af samarbejdet kunne forskerne udføre eksperimentet i en uhørt stor skala: eksperimentet involverede i alt 61 millioner Facebook-brugere.

Studiet af Bond et al. er exceptionelt fordi det kombinerer kvaliteterne ved big data og eksperimentel metode. Mange forskere gør derfor også en stor indsats for at etablere samarbejder med virksomheder og organisationer der kan give dem adgang til data, der ellers ville være utilgængelige. Men samarbejde med virksomheder om big data er ikke uden faldgruber. For det første kræver det ofte et betydeligt bureaukratisk benarbejde at etablere et samarbejde. For det andet, og mere principielt problematisk, er virksomheder og organisationer sjældent interesserede i forskning der stiller dem selv i et dårligt lys. Det kan betyde at nogle typer undersøgelser prioriteres på bekostning af andre, alene fordi de passer bedre til store teknologivirksomheders dagsordener. Eksempelvis konkluderede @bond2012 at Facebook-kampagnen havde en gunstig effekt på valgdeltagelse. Det er i sagens natur en flatterende konklusion for Facebook. Men det er uklart om forskerne havde haft samme frihedsgrader til at studere de negative konsekvenser af at bruge Facebook.

<!-- Systems that collect big data are purpose-built, and the purpose is never political science research. This holds true even for the most research-friendly data sources. -->


# Big data og forskningsdesign

Big data kan anvendes til alle typer empiriske politologiske forskningsspørgsmål, det være sig deskriptive eller forklarende, forudsigende eller kausale spørgsmål. Selvom vi endnu kun er begyndt at se disse anvendelser udfolde sig, har vi i de seneste 15 år fået erfaringer nok til at kunne pege på én vigtig rettesnor når vi anvender big data: forskningsdesign er stadig altafgørende [jf. @toshkov16, s. 13; @CG15]. I afsnittet her diskuterer vi aspekter af forskningsdesign som kræver særlig opmærksomhed når man arbejder med big data. Det drejer sig dels om forskellige typer biases, dels om vigtigheden af at definere sin analyseenhed.

## Velkendt sampling bias

Den måske mest vidtløftige idé om big data kom til udtryk i en nu berygtet artikel i *Wired Magazine* af tidsskriftets stifter, Chris Anderson [anderson08]: idéen om at big data eliminerer stikprøveproblemer fordi '*n*=alt'. Med andre ord, big data gør det muligt at analysere *alle data*, ikke blot en stikprøve. Artiklen har fået stor opmærksomhed og masser af kritik siden den udkom. I begyndelsen af 2020 havde artiklen mere end 2.000 citationer på Google Scholar. For at sætte det i perspektiv har Maurice Duvergers berømte bog *Political Parties*, kilden til den navnkundige Duverger's Lov, samlet lidt mere end 7.000 citationer siden den udkom i 1959.

<!-- ML: The citation counts here are not explicitly necessary, I just enjoy pointing out how many people have dunked on Chris Anderson's article. -->

Som begreb indebærer '*n*=alt' at big data kan aflæses uden forbehold: mønstrene i big data tegner et komplet billede af menneskelig adfærd. Sandheden er imidlertid at vi ikke med big data slipper for at bekymre os om selektionsproblemer. Selektionsproblemer opstår i alle situationer hvor observationer figurerer i data af grunde der hænger systematisk sammen med vores afhængige variabel. @AP08 illusterer problemet med et eksempel fra en stor amerikansk folkesundhedsundersøgelse, hvor respondenter i et survey bedes om at angive deres sundhedstilstand på en skala. Ikke overraskende konstaterer forskerne at personer der er indlagt på hospitalet konsekvens udviser et ringere helbred end personer der ikke er indlagt. Skal vi heraf konkludere at hospitaler gør folk mere syge? Svaret er indlysende nej. Det er indlysende, fordi vi i forvejen ved at individer selekterer sig ind i hospitaler netop fordi de er syge til at begynde med. Hvis man ignorerede dette selektionsproblem ville man ende med sampling bias.

Tilsvarende situationer opstår hele tiden i arbejdet med big data. For eksempel beretter @harford14 om den amerikanske by Bostons erfaringer med at bruge smartphonedata til at registrere asfalthuller i byens vejnet. En særlig app som indbyggere i Boston kunne installere registrerede bump i vejen ved hjælp af smartphonens indbyggede accelerometer, og kunne på den måde kortlægge byens asfalthuller. Resultatet af projektet var at byen opdagede alle asfalthuller i kvarterer hvor yngre, velstående bilister færdedes -- netop den type person som ville være mest tilbøjelig til at eje en smartphone og downloade byens app. @TSSW10 påpeger et lignende problem i tidligere forskning som har forsøgt at forudsige valgresultater med Twitter-data. Twitter-omtale forudsiger ganske rigtig partiopbakning glimrende ved Tysklands forbundsvalg i 2009 -- med én slående undtagelse. Det ekstremt online *Piratpartiet* fik en kolossal mængde Twitter-omtale men en meget lille andel af de faktiske stemmer. I begge disse eksempler gælder '*n*=alt' alene for en meget specifik, selvselekteret gruppe af særligt sofistikerede teknologibrugere.

Det har således altid været tilfældet at man risikerer at få de forkerte svar hvis man ikke tager højde for sampling bias. Hvis man bruger big data med en biased stikprøve får man plot et ekstremt præcist estimat af det forkerte svar.

## Særlige problemer ved big data

Big data confronts us with new sources of bias and confounding. Perhaps the most famous example for social scientists is that of Google Flu Trends (GFT). GFT was a Google project launched in 2008 that predicted regional flu epidemics from fine-grained data on users' Google queries about likely flu symptoms [@Getal2009]. After initially receiving attention for its impressive accuracy, GFT's predictive performance began to diminish over time until--by the time the project was shuttered in 2015--for years it had produced inaccurately high forecasts sometimes as great as double the number of flu cases reported by the U.S. Centers for Disease Control and Prevention [@harford14]. The reasons for GFT's collapse are instructive for understanding both why research design still matters and what new headaches come with big data.

Several things contributed to GFT's problems, but the most noteworthy seems to have arisen after Google adjusted its main search service [see @LKKV14]. Like any organization that produces or processes big data, Google changes its services or its algorithms from time to time. In 2011 and 2012, Google added functions to its search service that suggested additional search terms to users, based on their original search terms. It even suggested possible diagnoses for search terms that involved disease symptoms. The upshot was an apparent feedback loop: users of Google search were nudged to refine their searches for symptoms and to seek out information on the flu, intensifying the signals that GFT relied on to predict flu outbreaks. There is more to GFT's story, of course, but this part of it points out some special features of big data researchers must consider to avoid sampling bias. As @salganik17 puts it, big data is *algorithmically confounded*, *dirty*, and *drifting*.

Algorithmic confounding happens when computer behavior interacts with human behavior in a system, altering the human behavior we want to study. GFT is an example, since computer behavior--the Google search algorithm's recommendations--interacted with the human behavior that GFT relied on to predict flu outbreaks (searches for flu symptoms). Google's search suggestions altered users' perceptions of their symptoms and led them to different search behavior.

By "dirty," @salganik17 means that big data also contains false positives in the form of computer behavior that is mistaken for human behavior. For example, at least a portion of Google's additional search traffic around flu syptoms may have been purely accidental, caused only by the search engine's suggestions. Smart phone users will recognize this problem as an autocorrect fail. An example that occassionaly makes headlines would be the presence of large numbers of "bots" on social media platforms. Bots are accounts that are not operated by human users, but rather are simply software programs executing automated behaviors in the form of liking, following, or commenting. Despite Twitter and other platforms' occasional purges of bot accounts, an unspecified and probably high number of bots persist. Any analysis of social media data must think carefully about how to clean the data to avoid analyzing bot behavior as if it were human behavior.

Drift refers to the occasional changes that occur in how big data are collected, a problem we foreshadowed in the first section. GFT was a victim of the drift in Google's search data that occurred after the introduction of search suggestions in 2011. Fundamentally, GFT's data before and after 2011 came from different populations--one in which users were treated with nudges and the other without that treatment. Therefore, these data were impossible to compare. Take note. GFT was an internal project, and yet the GFT team missed changes that their Google search colleagues introduced. Researchers using big data must stay aware of how their data may have drifted.

## Overcoming biases in big data

These biases have no general cure, but adopting good practices can ensure you catch many problems before they become threats to the validity of your research design. First, get to know your data. If you collect your data from an API, read its documentation carefully. If you take it from a website, then understand everything you can about how the data on the site are maintained and curated--who puts it up, who can remove it, how it is updated, etc. Many organizations that produce and provide big data communicate about it, for example on company blogs or through press releases. Pay attention to these sources of information for things that can affect your research design. When in doubt, ask clarifying questions. Many system administrators or database managers are happy to help courteous researchers.

Second, try to write down your assumptions explicitly and, whenever you can, directly test them like hypotheses [see, @LBCC16]. For example, GFT could have tested for algorithmic confounding by examining whether feedback from the search engine to the user was associated with increases in symptom-related search terms. Social media analyses should test the assumption that their data are generated by humans by checking, for example, that individuals in the data do not post too repetitively, do not post inhumanly fast, and exhibit use patterns that are within some range of normal behavior for the platform.

Finally, develop a habit of running sanity checks, even when you do not anticipate problems and cannot formulate an explicit assumption. Since big data is, well, big, it is not realistic to look at the numbers or text in your data and expect to see anything useful. We do this by drawing pictures. Make scatterplots, histograms, and other plots of your variables and be sure the patterns make sense. When you discover a strange pattern, investigate it until you understand it. You may very well uncover hidden drift or the tell-tale patterns of dirty data caused by bots or other automated interference.

## Levels of analysis and measurements on big data

Theories in empirical political science come with a *level of analysis*. For example, a theory about party behavior during election campaigns has a party-election level of analysis: each party at each election is one observation. A theory about politicians during election campaigns gives a necessarily larger number of observations per election, since each individual politician during a campaign is one observation. It is relatively rare in political science to express our theories at the level of granularity in most big data sets. It is, of course, entirely possible to develop a theory to explain individual tweets or individual laws, etc., but political science generally expresses its theories at a higher level (less granular) level of analysis.

In the next section, we introduce several popular and useful methods for working with big data. As we will see, these methods specifically associated with big data have in common that they are actually measurement tools. We will look at methods for labeling data, discovering patterns, and condensing large amounts of data into simpler, smaller data. It is typical for political scientists applying big data that their "big" data set of millions of tweets or thousands of laws condenses into a data set of, for example, thousands of politician-month observations or hundreds of government-year observations.

Even when working with big data sets, the workhorse method of observational, quantitative hypothesis-testing in political science remains to build and estimate appropriate statistical models with appropriate control variables. Thus, we will focus on the methodological choices that distinguish analyses using big data from the more familiar social science workflow of building a purpose-built data set arranged at the correct level of analysis. Readers who are familiar with qualitative methods will be more familiar with this pattern. For example, lengthy interviews or wide-ranging case studies are often condensed into a summary coding of theoretical concepts.

This raises a point worth emphasizing about big data methods in political science. Although methods for working with big data have to be computational, they do not necessarily mean that researchers must apply quantitative methodology in their overall research design. Descriptive and qualitative research designs can equally well apply the methods we discuss below.



# Behandling af big data

Der er groft sagt to typer udfordringer der gør sig særligt gældende når man arbejder med big data. Der er for det første en mænge *computationelle* udfordringer, dvs. rent tekniske udfordringer med at bearbejde datamængder der er væsentligt større end hvad en almindelig computer typisk skal håndtere. Computationelle udfordringer er hyppige i arbejde med big data. Faktisk afgrænser den oprindelige definition i @laney01 netop big data til datamængder der overskrider en almindelig desktop-computers kapacitet. Det er også i praksis en relevant udfordring. Den første frigivelse af data fra Facebooks forskningssamarbejde *Social Science One* bygger eksempelvis på en omtrent en exabyte data, svarende til en milliard gigabytes. Vi dækker ikke håndteringen af computationelle udfordringer nærmere her, men henviser til @varian2014big, som giver en tilgængelig indføring i forskelle tekniske metoder til håndtering af kolossale datamængder.

Vi fokuserer her i stedet på særlige analytiske udfordringer der opstår når man arbejder med big data. Selv hvis vi ser bort fra den rent tekniske håndtering af data er der stadig særpræg ved big data som ofte gør gængse samfundsvidenskabelige metoder utilstrækkelige. Antag for eksempel at vi interesserer os for danske statsministres retorik med afsæt i et datasæt med statsministres nytårstaler mellem 1985 og 2020. Med bare 36 taler er datasættet egentlig ret småt, men ustrukturerede tekstdata af denne type er typiske i big data-analyser, og selv dette lille eksempel illustrerer de særlige analytiske udfordringer ved big data.

**make more explicit that this is mostly a text as data perspective**

## Dimensionalitetsreduktion

Vi kunne for eksempel spørge os selv hvilke ord socialdemokratiske statsministre bruger særligt hyppigt sammenlignet med borgerlige statsministre. En gængs samfundsvidenskabelig tilgang til sådan et problem ville være at estimere en regressionsmodel med statsministerens partitilhørsforhold som afhængig variabel og uafhængige variable der angiver hvert enkelt ords hyppighed i den enkelte tale. Men det kan ikke lade sig gøre. En almindelig regressionsmodel kan kun estimeres hvis antallet af observationer overstiger antallet af uafhængige variable i modellen, og det er ikke tilfældet her. Faktisk har danske statsministre brugt 7.286 forskellige ord i de 36 taler. Datasættet har altså godt og vel 200 gange så mange variable som der er observationer.

**MAYBE VISUALIZE DFM HERE**

Vi har med andre ord at gøre med ekstremt *højdimensionelle* data. Fordi al informationen i data er spredt ud over et stort antal variable er informationsværdien i hver enkelt variabel paradoksalt nok meget lille. Højdimensionalitet opstår ikke alene i arbejdet med tekstdata, men også med data om sociale netværk eller billeder. Derfor er *dimensionalitetsreduktion* et centralt analytisk mål i næsten alle big data-metoder. Dimensionalitetsreduktion indebærer at man reducerer antallet af variable, som i big data-terminologi ofte kaldes *features*, til et lille antal som indeholder mest mulig relevant information fra den oprindelige, fulde mængde af variable. Man kan tænke på dimensionalitetsreduktion som en måde at udtrække en information om en latent variabel fra et stort antal manifeste variable. Det svarer i princippet til forholdet mellem den teoretiske variabel og indikatorerne i et refleksivt indeks (jf. kapitel **XX**). Forskellen er blot at antallet af manifeste variable i big data er langt højere.

## Klassifikation og skalering

Det er det konkrete forskningsspørgsmål der afgør hvad der er 'relevant' information i det oprindelige datasæt, og dermed også hvilken metode til dimensionalitetsreduktion der er mest passende. Men et godt udgangspunkt er at gøre sig klart hvilken latent variabel man er interesseret i at måle. Karakteren af denne latente variabel vil være afgørende for hvilken konkret analysemetode man skal tage i brug. En særligt vigtig sondring er om variablen er nominal- eller intervalskaleret.

Hvis den latente variabel er intervalskaleret skal man bruge en metode til *skalering*. Her placerer man altså hver enkel enhed på en numerisk skala der kan fortolkes intervalskaleret. I eksemplet med statsministres nytårstaler kunne man interessere sig for om venstre-højre-ideologi kommer til udtryk i talerne. Her ville en skaleringsmetode placere alle talerne på en venstre-højre-dimension. Metoderne *Wordscores* og *Wordfish*, som præsenteres i kapitel **XX**, er begge skaleringsmetoder udviklet af politologer.

Hvis den latente variabel er nominalskaleret giver det per definition ikke mening at tilskrive hver enhed en numerisk værdi. Her har vi i stedet at gøre med *klassifikation*, dvs. at hver enhed tilknyttes en teoretisk bestemt kategori. I behandling af tekstdata **TODO: finish this**

TODO

## Superviserede og usuperviserede tilgange

TODO

<!-- - Pattern discovery (dimensionality reduction -- a la argument in Lowe 2013WP)
  - Generic data: Clustering, IRT, etc.
  - Fundamental unity of goals and approach
  - Variety in methods results from variation in:
    - Assumptions re: underlying model/geometry of latent space
    - Related to above: something like, "format" of output
    - Structure/nature of input data
    - Amount of domain expertise applied to structure results
    - Assumptions about what is correlated with what
    - Level of computational intensity -->




# Etiske problemer ved big data

emotional contagion example: @kramer2014experimental

problem: lack of informed consent

complication: under surveillance capitalism, all citizens are subject to constant experimentation w/o consent

something from Weapons of Math Destruction

algorithmic bias




A final ethical concern to consider when applying big data relates to transparency and human bias. As we saw above, methods for making measurements from big data take their results directly from the data. Supervised classification learns associations between features and the target outcome, while unsupervised methods mine multidimensional associations in the data to return scales, clusters, etc. It is valuable to keep in mind that simply learning the associations in data is not neutral because data are not neutral. They are produced in the real world, complete with all of its biases: from the quirks of human perception to systematic racial prejudice or social inequality.

Uncritically learning associations in data can mean doubling down on negative aspects of the real world. For example, @oneill16 tells the story of how credit card companies in the U.S. market their products online to consumers, based on something called an *e-score*. The e-score is calculated from big data, including a potential customer's browsing history, location, and other data points. As a result, for example, people browsing the Internet from poorer areas of the country, with higher loan default rates, would generally receive a worse e-score. Lower e-scores mean the person sees advertisements for credit cards with less favorable terms, reflecting their apparent risk for the card issuer. This pattern contributes to deepening economic inequality and even racial disparities in wealth when neighborhoods are strongly racially segregated.

Any algorithm that learns from patterns in real-world data on human behavior can and will pick up associations of this nature -- legacies of inequality, racial and gender prejudice, etc. This is particularly relevant for the study of political science and public administration. The state makes life-altering decisions each day, for example, social workers help decide whether to remove children from potentially abusive homes into protective state custody or and case workers help match unemployed job-seekers to employment opportunities. If one were to apply a classification model, trained on historical data, to assist these processes, that model would inherit all the systematic biases or inequalities that may have characterized that system.

This is where transparency is crucial. Whether using big data for research or applying it in business or government, when issues like fairness or prejudice matter, it is crucial that we examine our models for these types of biases. This means slowing down, examining a model's results for bias, and being transparent about what we find before rushing to apply it. Many methods applied to big data are difficult or impossible to directly examine unlike, for instance, the coefficients of a regression model [@blackbox]. Therefore, it is up to the analyst to probe the model for evidence of bias by studying what the model predicts or what measurements it returns under a variety of circumstances. Algorithmic transparency is an active research field in computer science, but unfortunately right now there is no methodological trick that can substitute for carefully and systematically examining our models' performance.



<!-- ## Skills -->

<!-- - Data collection / management -->
<!-- -  -->
