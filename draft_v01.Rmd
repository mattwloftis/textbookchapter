---
title: "'Big data' og politologisk datavidenskab"
author:
  - Frederik Hjorth
  - Matt W. Loftis
subtitle: "Udkast, januar 2020"
date:
  # "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
  word_document: default
biblio-style: apsr
fontfamily: mathptmx
fontsize: 12pt
geometry: margin=1in
bibliography: references.bib
spacing: double
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Draft motivation:

Training, job market demands, sexy research, etc. increasingly value the ability to work with big data. It's not always clear what one means by big data or how one works with it, but there is a consensus that doing so requires skills. We agree, and we present some of these skills here, along with a description of what big data means and in what ways we work with it.

cites: @mullainathan2017machine, @varian2014big

## Hvad er big data? 

Big data has been credited with enabling an emerging field of computational social science with "the capacity to collect and analyze data with an unprecedented breadth and depth and scale" [@lazeretal]. Yet, the concept of big data lives two lives. On the one hand the popular definition is associated with exciting and futuristic promises of data-driven science and technology. On the other hand is what we might call the operational definition of how massive data sets are collected, stored, and used currently by social scientists. To unpack the role of big data in political science research, we first consider the second aspect of the concept and then circle back to examine what to believe about the first aspect.

One widely familiar operational definition identifies big data with the so-called three Vs: Volume, Variety, and Velocity [@laney01]. Big data, here, refers to fast-evolving and very large data sets--including even data too large to store on a desktop computer--that contain lots of variation. Data sources of this nature are often interesting to social science: search engine logs, social media activity, government administrative records, mobile telephone records, or even data stored by passive monitoring conducted by digital devices in the physical world have been mobilized for social science research [@salganik17]. Accessing these data requires partnerships with their owners--phone companies, governments, technology companies, etc. This can mean writing software to access websites or remote databases or can involve more formalized partnerships to share information securely [@EL14].
<!-- ^[e.g. application programming interfaces (APIs)] -->

The hurdle of accessing big data underscores a basic difference between it and traditional social science data: traditional social science data were collected for the purpose of doing social science. Social scientists must always consider the sources and the nature of our data, and big data has sometimes been referred to as "found data" or "digital exhaust" [@harford14]. What does this mean? Virtually all big data are purpose-built for goals *other than* social research. Metadata like timestamps, follower counts, or activity frequencies on social media web sites are not stored for science or, necessarily, according to scientific standards. Although they may be useful, their scientific value is an unintended byproduct (i.e. exhaust) of business or government activity. As such, when applying big data to social science research we must always probe the implications of the data's purpose for our scientific applications.^[As @salganik17 puts it, the challenges and opportunities created by big data follow from asking why the data were collected.]


- How/why big data became what it is in the zeitgeist
  - Classic advantages: Big / always on
  - Classic disadvantages: metered/restricted access + expertise barrier



## First skill of working w/big data: *Research Design*

Systems that collect big data are purpose-built, and the purpose is never political science research. This holds true even for the most research-friendly data sources.

<!-- Include example about retsinformation.dk?? Ministry titles in official metadata reflect "current" institutional structure and throws out information contemporary to older data. This creates a massive headache for researchers, but it makes perfect sense for running an efficient bookkeeping operation for the central bureaucracy. Furthermore, there are not metadata linking laws to the decrees, regulations, etc. that they enable. This system would be superfluous and redundant since retsinformation is a broad-based clearing house and wouldn't necessarily be the tool of choice for legal advisors drafing secondary legislation within the line ministries. The data in the official gazette are complete, but they come with major biases caused directly by the fact that--even this public, non-partisan resource--is built with a purpose that is different from (not necessarily opposed to) political science research.

To cap it off, retsinformation.dk isn't finished evolving. It will continue to develop in ways that may or may not benefit political science research as its maintainers' and users' needs and preferences change.-->

- Big data typically *not designed for research*, i.e.
  - it's dirty (re: Salganik) -- human behavior is mixed together with actions taken by bots/automated systems
  - drifting -- Needs of *actual* system maintainers / users may be completely orthogonal to needs of political science research (or even opposed, consider FB)
  - algorithmically confounded -- large-scale systems have robot nannies. These include everything from spell checkers to YouTube's recommendation algorithm. The observed behavior we find in big data is a consequence of the (generally) unobservable interaction between humans and these algorithms.

- Standard sampling issues (nonrepresentative, systematic sampling bias)




## What to do with it:

- Pattern discovery (dimensionality reduction -- a la argument in Lowe 2013WP)
  - Generic data: Clustering, IRT, etc.
  - Text: Topic modeling, text scaling, dictionaries
  - Fundamental unity of goals and approach
  - Variety in methods results from variation in:
    - Assumptions re: underlying model/geometry of latent space
    - Related to above: something like, "format" of output
    - Structure/nature of input data
    - Amount of domain expertise applied to structure results
    - Assumptions about what is correlated with what
    - Level of computational intensity

- Classification / prediction

- Explanatory modeling



<!-- ## Skills -->

<!-- - Data collection / management -->
<!-- -  -->
